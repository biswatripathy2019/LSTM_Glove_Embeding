# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p6wXm--zka-3uFoz3cTGeitOaDWV0VLJ
"""

from keras.datasets import imdb
import pandas as pd
from keras.models import Sequential
import numpy as np
from keras.layers import LSTM, Activation, Dropout, Dense, Input
from keras.layers.embeddings import Embedding
from keras.models import Model
from keras.layers import Input,Dense,Embedding,SpatialDropout1D,add,concatenate
import string
import re
from keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelBinarizer
from keras.preprocessing.sequence import pad_sequences
import keras
from sklearn.model_selection import train_test_split
from nltk.stem.wordnet import WordNetLemmatizer
lem=WordNetLemmatizer()

from google.colab import files
files.upload()

data_all=pd.read_csv('/content/lstm_data_imdb.csv')

data=data_all[['Review','Sentiment']]
data.shape

from nltk.corpus import stopwords
#import nltk
#stop=stopwords.words('english')
#stop=set(nltk.corpus.stopwords.stop('english'))
import nltk
nltk.download('stopwords')
stop=stopwords.words('english')

stop

stopwords = [ "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", 
             "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during",
             "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", 
             "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into",
             "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or",
             "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", 
             "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's",
             "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up",
             "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's",
             "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've",
             "your", "yours", "yourself", "yourselves" ]

import nltk
nltk.download('wordnet')
replace_space=re.compile('[/(){}\[\]\!@,:;]')
bad_symbol=re.compile('[^0-9a-z #+_]')
def remove_stopwords(text):
  token=text.lower()
  token=re.sub('<.*?>','',token)
  token=replace_space.sub(' ',token)
  token=bad_symbol.sub('',token)
  token = [i for i in token if token not in stop]
  token=[lem.lemmatize(word) for word in token]
  token=''.join(token)
  return token

#data_without_stopwords = remove_stopwords(data)
data['clean_review']=data['Review'].apply(lambda x:remove_stopwords(x))
data['clean_review']=data['clean_review'].str.translate(string.punctuation)
#data['clean_output']=data['clean_output'].apply(lambda cw : remove_tags(cw))

data

data['clean_review'].head()

reviews = data['clean_review']
reviews

reviews_list = []
for i in range(len(reviews)):
  reviews_list.append(reviews[i])

reviews_list

sentiment = data['Sentiment']

x=reviews_list
y = np.array(list(map(lambda x: 1 if x=="positive" else 0, sentiment)))

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(x)

sequence=tokenizer.texts_to_sequences(x)
print('%s unique tokens ',len(sequence))

from google.colab import files
files.upload()

embedding_index=dict()
f=open(r'/content/glove.6B.50d.txt',encoding="utf8")
for line in f:
  values=line.split()
  word=values[0]
  coef=np.asarray(values[1:],dtype='float32')
  embedding_index[word]=coef
f.close()
print('word vectors %s' %len(embedding_index))

vocab_size=len(tokenizer.word_index)+1
vocab_size

embedding_matrix=np.zeros((vocab_size,300))
embedding_matrix.shape

for i,word in tokenizer.word_index.items():
  embeding_vector=embedding_index.get(word)
  if embeding_vector is not None:
    embedding_matrix[i]=embeding_vector
    print(embedding_matrix)

x=tokenizer.texts_to_sequences(x)
x=pad_sequences(x,maxlen=150)
x

y=pd.get_dummies(y).values

y

print('shape of x tensor %s',x.shape)

print('shape if y tensor',y.shape)

model=Sequential()
model.add(Embedding(vocab_size,300,weights=[embedding_matrix],input_length=x.shape[1],trainable=True))
model.add(SpatialDropout1D(0.30))
model.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])
print(model.summary())

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test,indices_train,indices_test=train_test_split(x,y,data_all['ivr_call_id'],test_size=0.2,random_state=42)
epochs=100
batch_size=20
hist=model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.3)
accr1=model.evaluate(x_test,y_test)
print('loss {} accuracy {}'.format(accr1[0],accr1[1]))

indices_train

print(model.predict(x_test))
print(y_test)

model.save('review.h5')

from keras.models import load_model
mod=load_model('/content/review.h5')
d=pd.read_csv('/content/lstm_data_imdb.csv')
d.head()

d['clean_review']=d['Review'].apply(lambda x:remove_stopwords(x))

d['clean_review']=d['clean_review'].str.translate(string.punctuation)

data.columns

tok=Tokenizer()
tok.fit_on_texts(data['clean_review'].tolist())
#print(x)
test_x=tokenizer.texts_to_sequences(d['clean_review'].values)
test_x=pad_sequences(test_x,maxlen=150)
print('shape of data tensor',test_x.shape)
pred=mod.predict_classes(test_x)
d['final_level']=pred
assign_tag={0:'negetive',1:'positive'}
d.to_csv('item_final_over.csv')

pred

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
import numpy as np
Y_test=np.argmax(y_test,axis=1)
Y_pred=model.predict_classes(x_test)

print(classification_report(Y_pred,Y_test))

ivrid_test=pd.DataFrame({'ivr_call_id':indices_test,'encoded_tag':Y_test})
get_tag=ivrid_test.merge(data_all)[['ivr_call_id','encoded_tag','Sentiment']]
assign_val=get_tag.set_index('encoded_tag')['Sentiment'].to_dict()
assign_val

reversed_test=tokenizer.sequences_to_texts(list(x_test))
target_num=pd.DataFrame(list(y_test)).idxmax(axis=1).tolist()
pred_num=list(Y_pred)
df_to_check_ori_pred=pd.DataFrame({'original':target_num,'predicted':pred_num})
assign_tag=assign_val
df_to_check_ori_pred=df_to_check_ori_pred.replace({'original':assign_tag,'predicted':assign_tag})
df_to_check_ori_pred['text']=reversed_test
df_to_check_ori_pred['rid']=indices_test.tolist()
df_to_check_ori_pred

cf=confusion_matrix(df_to_check_ori_pred.original,df_to_check_ori_pred.predicted)
ivrid=df_to_check_ori_pred.rid.tolist()

dcf=pd.DataFrame(cf)
dcf.columns=list(data_all['Sentiment'].unique())
dcf.index=list(data_all['Sentiment'].unique())
dcf['pred_test']=dcf.sum(axis=1)
dcf

assign_val
key_val=assign_val.items()
new_d={str(key):str(value) for key,value in key_val}
class_rep=pd.DataFrame(classification_report(Y_test,Y_pred,output_dict=True))
class_rep.rename(columns=new_d,inplace=True)
class_rep=class_rep
class_rep.T
#matrix_all=pd.concat

import matplotlib.pyplot as plt
plt.title('Loss')
plt.plot(hist.history['loss'],label='train')
plt.plot(hist.history['val_loss'],label='test')
plt.legend()
plt.show()
plt.title('Accuracy')
plt.plot(hist.history['accuracy'],label='train')
plt.plot(hist.history['val_accuracy'],label='test')
plt.legend()
plt.show()

